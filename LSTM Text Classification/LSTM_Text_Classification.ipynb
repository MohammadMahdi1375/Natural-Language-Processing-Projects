{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225cbe753df4423687b8798be546d68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/759 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphilschmid/flan-t5-xxl-sharded-fp16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# load model from the hub\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:464\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    463\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    465\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:2040\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2038\u001b[0m         )\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2040\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2041\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2042\u001b[0m         )\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit:\n\u001b[1;32m   2045\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n",
      "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'ham', 'text': ['wat', 'makes', 'some', 'people', 'dearer', 'is', 'not', 'just', 'de', 'happiness', 'dat', 'u', 'feel', 'when', 'u', 'meet', 'them', 'but', 'de', 'pain', 'u', 'feel', 'when', 'u', 'miss', 'dem', '!', '!', '!']}\n",
      "{'label': 'ham', 'text': ['sat', 'right', '?', 'okay', 'thanks', '...']}\n",
      "{'label': 'ham', 'text': ['when', 'you', 'and', 'derek', 'done', 'with', 'class', '?']}\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(init_token=\"<bos>\", eos_token=\"<eos>\", tokenize=lambda text: [token.text for token in nlp(text)], include_lengths=True, lower=True, batch_first=True)\n",
    "LABEL = torchtext.data.LabelField(sequential=False, dtype=torch.float, batch_first=True)\n",
    "fields = [('label', LABEL), ('text', TEXT)]\n",
    "\n",
    "data = torchtext.data.TabularDataset(path=\"/home/moh7596/Dataset/SMS_ Spam_Ham_Prediction/sms_spam.csv\",\n",
    "                                     format=\"CSV\",\n",
    "                                     fields=fields,\n",
    "                                     skip_header=True)\n",
    "\n",
    "training_data, validation_data, testing_data = data.split([0.7, 0.15, 0.15], random_state=random.seed(2023))\n",
    "\n",
    "training_iterator = torchtext.data.BucketIterator(dataset=training_data,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  sort_key=lambda x: len(x.text),\n",
    "                                                  device=DEVICE,\n",
    "                                                  shuffle=True,\n",
    "                                                  sort_within_batch=True,\n",
    "                                                  sort=False)\n",
    "                                                  \n",
    "validation_iterator = torchtext.data.BucketIterator(dataset=validation_data,\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    sort_key=lambda x: len(x.text),\n",
    "                                                    device=DEVICE,\n",
    "                                                    shuffle=True,\n",
    "                                                    sort_within_batch=True,\n",
    "                                                    sort=False)\n",
    "\n",
    "testubg_iterat = torchtext.data.BucketIterator(dataset=testing_data,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               sort_key=lambda x: len(x.text),\n",
    "                                               device=DEVICE,\n",
    "                                               shuffle=True,\n",
    "                                               sort_within_batch=True,\n",
    "                                               sort=False)\n",
    "\n",
    "print(vars(training_data.examples[0]))\n",
    "print(vars(validation_data.examples[0]))\n",
    "print(vars(testing_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary:  7816\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "vectors = torchtext.vocab.Vectors(name=\"/home/moh7596/Dataset/WordEmbedding/glove.6B/glove.6B.300d.txt\")\n",
    "\n",
    "TEXT.build_vocab(training_data, min_freq=1)\n",
    "LABEL.build_vocab(training_data)\n",
    "print(\"Size of TEXT vocabulary: \", len(TEXT.vocab))\n",
    "\n",
    "print(TEXT.vocab[\"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(400000, 300)\n",
       "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(LSTM, self).__init__()\n",
    "        #self.embedding = torch.nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "        #self.embedding.weight.data.copy_(vectors.vectors)\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(vectors.vectors)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = torch.nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc =  torch.nn.Linear(2*hidden_dim, output_dim)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, text_length):\n",
    "        x = self.embedding(text)\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, text_length, batch_first=True)\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        hidden = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
    "        hidden = self.fc(hidden)\n",
    "        x = self.sigmoid(hidden)\n",
    "\n",
    "        return x\n",
    "        \n",
    "        \n",
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0\n",
    "\n",
    "model = LSTM(vocab_size=vocab_size,\n",
    "             embedding_dim=embedding_dim,\n",
    "             hidden_dim=hidden_dim,\n",
    "             output_dim=output_dim,\n",
    "             n_layers=n_layers,\n",
    "             bidirectional=bidirectional,\n",
    "             dropout=dropout\n",
    "             ).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.BCELoss().to(DEVICE)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y_true):\n",
    "    #round y_hat to the closest integer (y_hat is an array of float numbers ranging from 0 to 1, y_true is an array of integer numbers 0 or 1)\n",
    "    return (torch.round(y_hat) == y_true).float().sum().item()\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    batch_acc = 0\n",
    "    num_data = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        ## That is why you we set \"include_length=True\", becasue this length is used in the bucket iterator and pack_padded_sequence\n",
    "        text, text_length = batch.text\n",
    "        text = text.to(DEVICE)\n",
    "        text_length = text_length.to('cpu')\n",
    "        y_predict = model(text, text_length).squeeze()\n",
    "        loss = criterion(y_predict, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy(y_predict, batch.label)\n",
    "\n",
    "        batch_acc += acc\n",
    "        epoch_loss += loss.item()\n",
    "        num_data += y_predict.shape[0]\n",
    "\n",
    "    return epoch_loss, batch_acc/num_data\n",
    "\n",
    "\n",
    "def evaluation(model, iterator, criterion):\n",
    "    num_data = 0\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        text, text_length = batch.text\n",
    "        text_length = text_length.to('cpu')\n",
    "        y_predict = model(text, text_length).squeeze()\n",
    "        \n",
    "        loss = criterion(y_predict, batch.label)\n",
    "        acc = accuracy(y_predict, batch.label)\n",
    "        test_loss += loss.item()\n",
    "        test_acc += acc\n",
    "\n",
    "        num_data += y_predict.shape[0]\n",
    "\n",
    "    return test_loss, test_acc/num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0 \t Train Loss: 61.626 \t Test loss: 10.635 \t Test Accuracy: 0.859\n",
      "EPOCH:  1 \t Train Loss: 46.873 \t Test loss: 10.315 \t Test Accuracy: 0.859\n",
      "EPOCH:  2 \t Train Loss: 44.244 \t Test loss: 9.699 \t Test Accuracy: 0.859\n",
      "EPOCH:  3 \t Train Loss: 36.302 \t Test loss: 6.562 \t Test Accuracy: 0.859\n",
      "EPOCH:  4 \t Train Loss: 25.484 \t Test loss: 5.289 \t Test Accuracy: 0.932\n",
      "EPOCH:  5 \t Train Loss: 18.486 \t Test loss: 4.431 \t Test Accuracy: 0.947\n",
      "EPOCH:  6 \t Train Loss: 16.181 \t Test loss: 4.370 \t Test Accuracy: 0.947\n",
      "EPOCH:  7 \t Train Loss: 12.852 \t Test loss: 4.058 \t Test Accuracy: 0.957\n",
      "EPOCH:  8 \t Train Loss: 11.420 \t Test loss: 3.465 \t Test Accuracy: 0.963\n",
      "EPOCH:  9 \t Train Loss: 10.955 \t Test loss: 4.997 \t Test Accuracy: 0.952\n",
      "EPOCH: 10 \t Train Loss: 9.190 \t Test loss: 3.289 \t Test Accuracy: 0.963\n",
      "EPOCH: 11 \t Train Loss: 7.974 \t Test loss: 3.861 \t Test Accuracy: 0.964\n",
      "EPOCH: 12 \t Train Loss: 8.827 \t Test loss: 3.649 \t Test Accuracy: 0.963\n",
      "EPOCH: 13 \t Train Loss: 7.121 \t Test loss: 3.913 \t Test Accuracy: 0.963\n",
      "EPOCH: 14 \t Train Loss: 5.991 \t Test loss: 3.247 \t Test Accuracy: 0.970\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 15\n",
    "for epoch in range(EPOCH):\n",
    "    train_loss, train_acc = train(model, training_iterator, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluation(model, testubg_iterat, criterion)\n",
    "\n",
    "    print(f\"EPOCH: %2d \\t Train Loss: %.3f \\t Test loss: %.3f \\t Test Accuracy: %.3f\" % (epoch, train_loss, test_loss, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pyth310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
