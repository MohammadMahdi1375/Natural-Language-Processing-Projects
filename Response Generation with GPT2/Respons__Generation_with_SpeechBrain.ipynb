{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI6a5KiMQWma"
      },
      "source": [
        "# **Tutorial on Response Generation with SpeechBrain**\n",
        "\n",
        "\n",
        "This tutorial will guide you through the process of **fine-tuning the pretrained GPT2 model** that is available in the HuggingFace Transformers library for response generation.\n",
        "\n",
        "\n",
        "**What is a dialogue system?**\n",
        "\n",
        "In previous labs, we have implemented machine translation, which is used to read the source language (input) and generate the desired language (output). Similarly, in a dialogue system, we will implement a model to generate a response given a context. This is also known as Natural Language Generation (NLG).\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yef6QgRpT1ktP6BpHelPmA.png\" alt=\"drawing\" width=\"700\" align=\"center\"/>\n",
        "\n",
        "\n",
        "**Transformers for Language Modeling**\n",
        "\n",
        "As we’ve seen in the previous labs, the original transformer model is made up of an encoder and decoder – each is a stack of what we can call transformer blocks. A lot of the subsequent research works try to focus only on either the encoder or decoder, and use just one stack of transformer blocks – stacking them up as high as practically possible and feeding them massive amounts of training text.\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/gpt2/gpt-2-transformer-xl-bert-3.png\" alt=\"drawing\" width=\"700\" align=\"center\"/>\n",
        "\n",
        "How high can we stack up these blocks? It turns out that’s one of the main distinguishing factors between the different GPT2 model sizes:\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/gpt2/gpt2-sizes-hyperparameters-3.png\" alt=\"drawing\" width=\"700\" align=\"center\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Architectures of interest for this tutorial**\n",
        "\n",
        "We will consider the smallest pre-trained GPT2 model : GPT-2.\n",
        "\n",
        "GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: **predict the next word, given all of the previous words within some text**. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data. Please refer to the official paper to obtain more details: [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).\n",
        "\n",
        "\n",
        "You could find some helpful resources here:\n",
        "\n",
        "*   [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)\n",
        "*   [How to build a State-of-the-Art Conversational AI with Transfer Learning](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)\n",
        "*   [Fun Article about Fine-Tuning for Superhero Descriptions](https://towardsdatascience.com/unleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4#:~:text=By%20fine-tuning%20GPT-3%2C%20creating%20a%20highly%20customized%20and,code%20and%20without%20assuming%20prior%20knowledge%20about%20GPT-3.)\n",
        "\n",
        "*  [GPT vs Bert](https://medium.com/@10shubhamkedar10/gpt-vs-bert-12d108956260)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**With this tutorial, you will learn how to:**\n",
        "\n",
        "1. Instantiate a pretrained GPT2.\n",
        "2. Fine-tuning GPT2 on MultiWOZ with SpeechBrain for response generation task.\n",
        "\n",
        "\n",
        "\n",
        "Let's first install all the needed packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12izOP1EZjfU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/speechbrain/speechbrain.git\n",
        "%cd speechbrain\n",
        "!pip install -r requirements.txt\n",
        "!pip install .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5x89dTJaytG"
      },
      "source": [
        "We also need to install the HuggingFace Transformers interface:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV3AoZCkbDTR",
        "outputId": "5c9a9303-ab13-41c4-9961-6f2235a83f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMWLjuaNMItz"
      },
      "source": [
        "## **1. Generate Texts  with GPT-2**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello, I am Speech Brain,\", max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368,
          "referenced_widgets": [
            "ce8ecaca819946efa0ac9465b9d286b7",
            "49078ad2bc544d6dae3721d42db4e799",
            "956383078d1a4dc6884224e9a3aa6fab",
            "f4ebd9dc90a04fcdb238d1ab2161259c",
            "bcb1fae90575471f93203330e0367f7a",
            "693f3bbebd934047b1ed9d552ed600c1",
            "7bc1a52ce9114ef5aed15f4e521e8a45",
            "be7f6f18ace04eb9aace3e31dc2b1167",
            "c1929deabceb4defad6c1a259d24468e",
            "e706f0e4f11b45bd92794784bb5b3d26",
            "2fa8f6f4d23c425c8a27c3465a8e10f0",
            "875725309be04a70b6f025e736dff5f5",
            "3e07b4d205694e308b105645d74117fc",
            "b1a3f7079a7c402cbe8244493182bdd8",
            "847f4a14a91f4464833c588161ca5244",
            "07fc7eb76d5a467e914bdc566e6ecd35",
            "59bd8095e86f44ee9c8817019be6b1c5",
            "fef6daf649394c2cbbdc6afcaced9b98",
            "7fbe4239e38f40e0beda29d545dc10bf",
            "8bd2b388ed9640f8b1d9755f97201f4d",
            "1810dbe227f846d9bf0e692e90adc7ea",
            "0b0a37786fb04495913c99003bc9cbde",
            "8e3b538431294564a982053fdf691d23",
            "9a03750595da463d8066363c693a68d6",
            "632eec3684f64d22805afa3fbddaf590",
            "7e0858de68574b50bc477033fc761451",
            "e6105e27ee834a4395be7a17ad2162ac",
            "08d3ddeca7d740d3b5ad15063d003a36",
            "c36d4821b70840888cc31d7cbb433e4a",
            "33c2c3f0725147b7b807a5e4915aa5d5",
            "f5facd634842444e8297d232335cab46",
            "ee18511d180440a18386adba30df1b8e",
            "14ba8a720e5a46198c2e3c40271e4a54",
            "687628988b704f13bab9fac9a9689ce8",
            "0b39335dcf444009950a9f06e4702601",
            "b20e41ac654543138e0b09e6646e7aea",
            "aebfb624015545b5af04f3a396cdd11b",
            "8f8704087f644f968eb95a320dbc0d1a",
            "4c52b02bc2bb464ebb88af8708fb4edf",
            "9656595fcf4941868c9b2f282440a34d",
            "04e24e29af1544abab400be534d371a0",
            "e9dc64c0e9864647a7e9dac8c1c6f35a",
            "7f3c7742c2684058b646e321778123dc",
            "67a4021781d34bb18f71108334e2fe6f",
            "b770c7693b52474f9b0fdd1f119488fe",
            "fa4915d2205d4b20ab9806bf54efb282",
            "e97c8d147622417dad88874890eb887d",
            "d522e6ed913b4a98920042a5c74117b0",
            "3c4bb66b670a4ca99677826b3df208a2",
            "0d5f12c1b6fb4301b0722ba589c6885c",
            "78586f3a880f445493444e263b2384b5",
            "d5fe559c3bfe43bf9653fe6ea14f6436",
            "a0793951f79b48c1a37f3740224e2872",
            "3bee5bddcbfc4fefaddf3a230a0b9a00",
            "d2a9a8c9b4ef41e5b79073d8729e9df2",
            "c9329c9e521f4412bf0e6b4bc889724a",
            "a53fb1b37e0c4a2198601624656e8d4a",
            "e17d906d143442f192e6a8aa031f7403",
            "3f8cfa75ed6a40458845dc5b4f6d9ca8",
            "02fa4e4c6c8046d182e4798aca67c1fe",
            "9b86e460d3db4293b7ba4d13570aa3b4",
            "faf66d5252d645959b594e0576af4ff9",
            "7b8b2285ed1f4e80bb95345a9c1c8fbe",
            "10f822100a564d4fb75284a91d70e509",
            "51910981257a4fccba669e5b8e96b571",
            "0977720d918e4df7b79a310ff05a942e"
          ]
        },
        "id": "VVNw9XPPmB2v",
        "outputId": "a6ae9d6a-377d-421c-94a8-f38d1d419844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce8ecaca819946efa0ac9465b9d286b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "875725309be04a70b6f025e736dff5f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e3b538431294564a982053fdf691d23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "687628988b704f13bab9fac9a9689ce8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b770c7693b52474f9b0fdd1f119488fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9329c9e521f4412bf0e6b4bc889724a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Hello, I am Speech Brain, I am a Computer Science Ph.D. student working on a PhD dissertation (Sections) on Artificial Intelligence'},\n",
              " {'generated_text': 'Hello, I am Speech Brain, and I am here to share an interesting book. This new book by Prof. Christopher Kallen, called What'},\n",
              " {'generated_text': 'Hello, I am Speech Brain, the world-famous speaker who inspired me. What can you tell me about you?\"\\n\\n\"No, it'},\n",
              " {'generated_text': 'Hello, I am Speech Brain, a writer specializing in cognitive neuropsychology and psychology and a leading researcher on a variety of domains of speech recognition,'},\n",
              " {'generated_text': 'Hello, I am Speech Brain, the most creative and imaginative of our community as you all know by now. But we are really not done yet,'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGUWY37XUb9t"
      },
      "source": [
        "Here, we can explore the model with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUOO9DAaUbPW",
        "outputId": "2b6b2f0f-a48b-4b72-8e8f-c2ae22a66632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(generator.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psdnsUwfsGgf"
      },
      "source": [
        "\n",
        "## **2. Pretrain GPT-2 and Fine-tune**\n",
        "Until now, we only saw how to use pre-trained GPT-2 to continue our own sentences.\n",
        "As we have learned in previous labs, the suggested way to use SpeechBrain is to directly plug your pre-trained model into your pipeline to fine-tune it while training our final model.\n",
        "\n",
        "\n",
        "Remember in \"Week 7: Pretrained and fine-tune\" lab, Wav2vec2 is offered as a **lobe** in SpeechBrain. Its implementation can be found in `speechbrain.lobes.models.huggingface_wav2vec.py`.  We need to have a similar interface for GPT. Then, HuggingFaceGPT can simply be added as a block to your hyper-params file:\n",
        "\n",
        "For GPT-2:\n",
        "```yaml\n",
        "GPT2: !new:HuggingFaceGPT\n",
        "    source: !ref <GPT_hub>\n",
        "    freeze: True\n",
        "    save_path: !ref <save_folder>/GPT_checkpoint\n",
        "```\n",
        "\n",
        "- *freeze* enables you to fine-tune (False) or freeze (True) the neural parameters while training your final model.\n",
        "\n",
        "The GPT-2 model is just a neural network that can be applied to your input data and can be jointly trained with the downstream task of interest.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file huggingface_GPT.py\n",
        "\n",
        "import logging\n",
        "\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class HuggingFaceGPT(nn.Module):\n",
        "    \"\"\"This lobe enables the integration of HuggingFace pretrained GPT model.\n",
        "      Arguments\n",
        "    ---------\n",
        "    source : str\n",
        "        HuggingFace hub name: e.g \"gpt2\"\n",
        "    save_path : str\n",
        "        Path (dir) of the downloaded model.\n",
        "    freeze : bool (default: False)\n",
        "        If True, the model is frozen. If False, the model will be trained\n",
        "        alongside with the rest of the pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, source: str, save_path: str, freeze: bool = False) -> None:\n",
        "        super().__init__()\n",
        "        self.freeze = freeze\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(source, cache_dir=save_path)\n",
        "        if self.freeze:\n",
        "            logger.warning(\n",
        "                \"huggingface_GPT - GPT  is frozen.\"\n",
        "            )\n",
        "            self.model.train()  # we keep it to train to have dropout and LN computed adequaly\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "\n",
        "    def forward(self, input_ids: Tensor, token_type_ids: Tensor):\n",
        "        with torch.set_grad_enabled(not self.freeze):\n",
        "            output = self.model.forward(input_ids, token_type_ids=token_type_ids)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "37pIyya_7A0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f58d37-427a-4d2a-e826-dc22b91aa75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing huggingface_GPT.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inputs of GPT2 model are input_ids and token_type_ids. Input_ids are a concatenation of all tokenized histories with the <speaker_token> of each sentence added before it. The token_type_ids has the same length as the input_ids and indicates who is the speaker of each token.\n",
        "For example:\n",
        "```\n",
        "history: 'Hi how are you', 'I am fine and you', 'I am good']\n",
        "input_ids : <speaker_1> Hi how are you <speaker_2> I am fine and you? <speaker_1> I am good>\n",
        "token_type_ids : [[<speaker_1>,<speaker_1>,<speaker_1>,<speaker_1>],\n",
        "                  [<speaker_2>,<speaker_2>,<speaker_2>,<speaker_2>,<speaker_2>],\n",
        "                  [<speaker_1> <speaker_1>,<speaker_1>]]\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "6C3pPu-665xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** It is just an illustrating example. The real input has the token_id instead of the words."
      ],
      "metadata": {
        "id": "VUnDyhuc_DrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Fine-tuning GPT-2 on Resonse Generation (with MultiWOZ)\n",
        "Now we will discuss how to fine-tune the GPT-2  model for response generation. To achieve this, we will be using a smaller version of the MultiWOZ 2.1 dataset. Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. Instead of using all the data, we will set a parameter that identifies the percentage of data to be sampled. For this experiment, we will just sample 1000, 100, 200 of training , valid and test enties.\n"
      ],
      "metadata": {
        "id": "tayhoDdV6SNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Prepare your data**\n",
        "The goal of data preparation is to create the data manifest files.\n",
        "These files tell SpeechBrain where to find the dialogue history and the system reply. They are text files written in the popular CSV and JSON formats.\n",
        "\n",
        "#### **Data manifest files**\n",
        "Let's take a look into how a data manifest file in JSON format looks like:\n",
        "\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"SNG01919.json_1\": {\n",
        "        \"history\": [\n",
        "            \"i need a taxi from the missing sock and i need to get to my destination by 08:30 . can you help ?\"\n",
        "        ],\n",
        "        \"reply\": \"i can help you with that . where are you going ?\",\n",
        "        \"length\": 145\n",
        "    },\n",
        "    \"SNG01919.json_3\": {\n",
        "        \"history\": [\n",
        "            \"i need a taxi from the missing sock and i need to get to my destination by 08:30 . can you help ?\",\n",
        "            \"i can help you with that . where are you going ?\",\n",
        "            \"i am going to el shaddai\"\n",
        "        ],\n",
        "        \"reply\": \"okay your booking is complete . be on the lookout for a white volkswagen\",\n",
        "        \"length\": 128.33333333333334\n",
        "    },\n",
        "    \"SNG01919.json_5\": {\n",
        "        \"history\": [\n",
        "            \"i need a taxi from the missing sock and i need to get to my destination by 08:30 . can you help ?\",\n",
        "            \"i can help you with that . where are you going ?\",\n",
        "            \"i am going to el shaddai\",\n",
        "            \"okay your booking is complete . be on the lookout for a white volkswagen\",\n",
        "            \"i will also need the contact number please .\"\n",
        "        ],\n",
        "        \"reply\": \"their contact number is 07053289961 . do you need any further assistance ?\",\n",
        "        \"length\": 131\n",
        "    }\n",
        "}\n",
        "```\n",
        "As you can see, we have a hierarchical structure in which the first key is a **unique identifier** of the name_of the dialouge+the turn number.\n",
        "\n",
        "You can specify here the entries with the name you prefer. However, there must be a matching between the name of these entries and what the experiment script (e.g, train.py) expects. We will elaborate more on this later.\n",
        "\n",
        "\n",
        "#### **Preparation Script**\n",
        "Every dataset is formatted in a different way. The script that parses your own dataset and creates the JSON or the CSV files is something that you are supposed to write. Most of the time, this is very straightforward.\n",
        "\n",
        "For the MultiWOZ dataset, for instance, we wrote this data preparation script called multiwoz_prepare.py.\n",
        "The function automatically downloads the data. We search for all the dialogues and split them based on the turns. Our goal is to train a model that could produce reasonable system-generated responses. Therefore, as our gold labels, we extract the system turns (replies uttered by the system). It will be our reply field in manifest files. Then, we will extract the history for each reply. The history is a list of sentences prior to that response. Each sentence is uttered either by a system or a user. You could see as we go further in the dialogue, the history become bigger since it contains all previous histories + new one. Even rows in history are uttered by the user and odd rows by the system. For the length, we take an average over the length of all sentences( number of words in each sentence). This field is only used by the data loader to sort data to avoid any unnecessary padded_tokens. It is not the actual length. The actual length of the inputs is varied depending on the tokenizer and token_type that we will use.\n",
        "\n",
        "You can use this script as a good base for your custom preparation on your target dataset. As you can see, we create three separate data manifest files to manage the training, validation, and test phases.\n",
        "\n"
      ],
      "metadata": {
        "id": "fUYeE7JcDfJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a good practice to make your data as clean as possible before feeding it to the model. To prepare the text data for the model building, we perform text preprocessing. Some of the preprocessing steps are:\n",
        "Removing punctuations like. , ! $( ) * % @\n",
        "Removing URLs\n",
        "Lower/upper casing\n",
        "Mapping abbreviations and short forms to their full forms (e.g. \"it's\" to \"it is\")\n",
        "We will apply these preprocessing steps using multiwoz_prepare.py and mapping.pair files."
      ],
      "metadata": {
        "id": "LT_xFZPcUf_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file mapping.pair\n",
        "it's\tit is\n",
        "don't\tdo not\n",
        "doesn't\tdoes not\n",
        "didn't\tdid not\n",
        "you'd\tyou would\n",
        "you're\tyou are\n",
        "you'll\tyou will\n",
        "i'm\ti am\n",
        "they're\tthey are\n",
        "that's\tthat is\n",
        "what's\twhat is\n",
        "couldn't\tcould not\n",
        "i've\ti have\n",
        "we've\twe have\n",
        "can't\tcannot\n",
        "i'd\ti would\n",
        "i'd\ti would\n",
        "aren't\tare not\n",
        "isn't\tis not\n",
        "wasn't\twas not\n",
        "weren't\twere not\n",
        "won't\twill not\n",
        "there's\tthere is\n",
        "there're\tthere are\n",
        ". .\t.\n",
        "restaurants\trestaurant -s\n",
        "hotels\thotel -s\n",
        "laptops\tlaptop -s\n",
        "cheaper\tcheap -er\n",
        "dinners\tdinner -s\n",
        "lunches\tlunch -s\n",
        "breakfasts\tbreakfast -s\n",
        "expensively\texpensive -ly\n",
        "moderately\tmoderate -ly\n",
        "cheaply\tcheap -ly\n",
        "prices\tprice -s\n",
        "places\tplace -s\n",
        "venues\tvenue -s\n",
        "ranges\trange -s\n",
        "meals\tmeal -s\n",
        "locations\tlocation -s\n",
        "areas\tarea -s\n",
        "policies\tpolicy -s\n",
        "children\tchild -s\n",
        "kids\tkid -s\n",
        "kidfriendly\tkid friendly\n",
        "cards\tcard -s\n",
        "upmarket\texpensive\n",
        "inpricey\tcheap\n",
        "inches\tinch -s\n",
        "uses\tuse -s\n",
        "dimensions\tdimension -s\n",
        "driverange\tdrive range\n",
        "includes\tinclude -s\n",
        "computers\tcomputer -s\n",
        "machines\tmachine -s\n",
        "families\tfamily -s\n",
        "ratings\trating -s\n",
        "constraints\tconstraint -s\n",
        "pricerange\tprice range\n",
        "batteryrating\tbattery rating\n",
        "requirements\trequirement -s\n",
        "drives\tdrive -s\n",
        "specifications\tspecification -s\n",
        "weightrange\tweight range\n",
        "harddrive\thard drive\n",
        "batterylife\tbattery life\n",
        "businesses\tbusiness -s\n",
        "hours\thour -s\n",
        "one\t1\n",
        "two\t2\n",
        "three\t3\n",
        "four\t4\n",
        "five\t5\n",
        "six\t6\n",
        "seven\t7\n",
        "eight\t8\n",
        "nine\t9\n",
        "ten\t10\n",
        "eleven\t11\n",
        "twelve\t12\n",
        "anywhere\tany where\n",
        "good bye\tgoodbye\n"
      ],
      "metadata": {
        "id": "1_QAROTCW0s3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24daa9ae-3b53-4d9d-a16d-0b3fe136a4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapping.pair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "from pathlib import Path\n",
        "from statistics import mean\n",
        "from typing import Any, Dict, List, Optional, Set, Tuple\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from speechbrain.utils.data_utils import download_file\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "MULTIWOZ_21_DATASET_URL = (\n",
        "    \"https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.1.zip\"\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "Trade script used for tokenization porposes.\n",
        "\n",
        "The original one can be found at:\n",
        "https://github.com/jasonwu0731/trade-dst/blob/master/create_data.py\n",
        "\"\"\"\n",
        "\n",
        "def prepare_mwoz_21(\n",
        "    output_folder: str,\n",
        "    data_folder: str,\n",
        "    override: bool,\n",
        "    replacements_path: str,\n",
        "    tr_random_dialogues: Optional[int] = None,\n",
        "    dev_random_dialogues: Optional[int] = None,\n",
        "    te_random_dialogues: Optional[int] = None,\n",
        "    seed: int = 42,\n",
        ") -> None:\n",
        "    # set seed\n",
        "    random.seed(seed)\n",
        "\n",
        "    dataset_folder = os.path.join(data_folder, \"MultiWOZ_21\")\n",
        "    if not os.path.isdir(dataset_folder):\n",
        "        download_mwoz_21(data_folder)\n",
        "    else:\n",
        "        logger.info(f\"{dataset_folder} exists, skipping.\")\n",
        "\n",
        "    tr_split, dev_split, te_split = get_splits(dataset_folder)\n",
        "\n",
        "    data_path = os.path.join(dataset_folder, \"data.json\")\n",
        "    build_dialogue_dataset(\n",
        "        data_path,\n",
        "        logger,\n",
        "        tr_split,\n",
        "        \"train.json\",\n",
        "        output_folder,\n",
        "        override,\n",
        "        replacements_path,\n",
        "        tr_random_dialogues,\n",
        "    )\n",
        "\n",
        "    build_dialogue_dataset(\n",
        "        data_path,\n",
        "        logger,\n",
        "        dev_split,\n",
        "        \"valid.json\",\n",
        "        output_folder,\n",
        "        override,\n",
        "        replacements_path,\n",
        "        dev_random_dialogues,\n",
        "    )\n",
        "\n",
        "    build_dialogue_dataset(\n",
        "        data_path,\n",
        "        logger,\n",
        "        te_split,\n",
        "        \"test.json\",\n",
        "        output_folder,\n",
        "        override,\n",
        "        replacements_path,\n",
        "        te_random_dialogues,\n",
        "    )\n",
        "\n",
        "\n",
        "def insertSpace(token, text):\n",
        "    sidx = 0\n",
        "    while True:\n",
        "        sidx = text.find(token, sidx)\n",
        "        if sidx == -1:\n",
        "            break\n",
        "        if (\n",
        "            sidx + 1 < len(text)\n",
        "            and re.match(\"[0-9]\", text[sidx - 1])\n",
        "            and re.match(\"[0-9]\", text[sidx + 1])\n",
        "        ):\n",
        "            sidx += 1\n",
        "            continue\n",
        "        if text[sidx - 1] != \" \":\n",
        "            text = text[:sidx] + \" \" + text[sidx:]\n",
        "            sidx += 1\n",
        "        if sidx + len(token) < len(text) and text[sidx + len(token)] != \" \":\n",
        "            text = text[: sidx + 1] + \" \" + text[sidx + 1 :]\n",
        "        sidx += 1\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize(text, replacements):\n",
        "    # lower case every word\n",
        "    text = text.lower()\n",
        "\n",
        "    # replace white spaces in front and end\n",
        "    text = re.sub(r\"^\\s*|\\s*$\", \"\", text)\n",
        "\n",
        "    # hotel domain pfb30\n",
        "    text = re.sub(r\"b&b\", \"bed and breakfast\", text)\n",
        "    text = re.sub(r\"b and b\", \"bed and breakfast\", text)\n",
        "\n",
        "    # weird unicode bug\n",
        "    text = re.sub(\"(\\u2018|\\u2019)\", \"'\", text)\n",
        "\n",
        "    # replace st.\n",
        "    text = text.replace(\";\", \",\")\n",
        "    text = re.sub(\"$\\/\", \"\", text)\n",
        "    text = text.replace(\"/\", \" and \")\n",
        "\n",
        "    # replace other special characters\n",
        "    text = text.replace(\"-\", \" \")\n",
        "    text = re.sub('[\"\\<>@\\(\\)]', \"\", text)  # remove\n",
        "\n",
        "    # insert white space before and after tokens:\n",
        "    for token in [\"?\", \".\", \",\", \"!\"]:\n",
        "        text = insertSpace(token, text)\n",
        "\n",
        "    # insert white space for 's\n",
        "    text = insertSpace(\"'s\", text)\n",
        "\n",
        "    # replace it's, does't, you'd ... etc\n",
        "    text = re.sub(\"^'\", \"\", text)\n",
        "    text = re.sub(\"'$\", \"\", text)\n",
        "    text = re.sub(\"'\\s\", \" \", text)\n",
        "    text = re.sub(\"\\s'\", \" \", text)\n",
        "    for fromx, tox in replacements:\n",
        "        text = \" \" + text + \" \"\n",
        "        text = text.replace(fromx, tox)[1:-1]\n",
        "\n",
        "    # remove multiple spaces\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "\n",
        "    # concatenate numbers\n",
        "    tokens = text.split()\n",
        "    i = 1\n",
        "    while i < len(tokens):\n",
        "        if re.match(\"^\\d+$\", tokens[i]) and re.match(\"\\d+$\", tokens[i - 1]):\n",
        "            tokens[i - 1] += tokens[i]\n",
        "            del tokens[i]\n",
        "        else:\n",
        "            i += 1\n",
        "    text = \" \".join(tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_replacements(\n",
        "    replacements_path: str = \"trade/utils/mapping.pair\",\n",
        ") -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Get the replacements from a given file. Used by trade preprocessing.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    replacements_path: str\n",
        "        File containing from, to pairs, one per line.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    replacements: List of replacements, i.e. pairs of str\n",
        "        Pairs of elements used to substitute the first element with the second.\n",
        "    \"\"\"\n",
        "    replacements = []\n",
        "    with open(replacements_path, \"r\") as fin:\n",
        "        for line in fin.readlines():\n",
        "            tok_from, tok_to = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
        "            replacements.append((\" \" + tok_from + \" \", \" \" + tok_to + \" \"))\n",
        "    return replacements\n",
        "\n",
        "\n",
        "TOKEN_EXCEPTIONS = {\"childs\": \"children\", \"businesss\": \"businesses\", \"inchs\": \"inches\"}\n",
        "\n",
        "PATTERN_EXCEPTIONS = {\"breakfasts\": \"b&bs\"}\n",
        "\n",
        "\n",
        "def invert_trade_subtokenization(\n",
        "    original_seq: str,\n",
        "    trade_seq: str,\n",
        "    token_exceptions: Dict[str, str] = TOKEN_EXCEPTIONS,\n",
        "    pattern_exceptions: Dict[str, str] = PATTERN_EXCEPTIONS,\n",
        "    subtoken_special_chrs: List[str] = [\" -\", \" _\"],\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Invert all trade subtokenizations in a string given the original sequence.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    original_seq: str\n",
        "        The original sequence.\n",
        "    trade_seq: str\n",
        "        The sequence that has been pre-processed by trade.\n",
        "    token_exceptions: dict, keys are str, values are str\n",
        "        A dictionary to map merged token to their correct counterpart. E.g.\n",
        "        child -s is merged into childs, but the correct token is children.\n",
        "    pattern_exceptions: dict, keys are str, values are str\n",
        "        A dictionary to map patterns to their correct counterpart. E.g.\n",
        "        after the pre-processing \"b&bs\" is mapped to \"bed and breakfast -s\",\n",
        "        making the search of breakfasts impossible if not handled by such\n",
        "        exceptions.\n",
        "    subtoken_special_chrs: list of str\n",
        "        List containing the special characters that are used for subtokens.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    corrected_seq: str\n",
        "        The sequence corrected, i.e. subtokens replaced by tokens.\n",
        "    \"\"\"\n",
        "    regex = \"|\".join(subtoken_special_chrs)\n",
        "    subtoken_pieces = re.split(regex, trade_seq, maxsplit=1)\n",
        "    search_after: int = 0\n",
        "    while len(subtoken_pieces) > 1:\n",
        "        # example: 'the wind is moderate -ly strong'\n",
        "        # split: ['the wind is moderate ', 'ly strong']\n",
        "        # split[0]: 'the wind is moderate' --> split on whitespace ['the', 'wind', 'is', 'moderate']\n",
        "        left_side = subtoken_pieces[0].split()\n",
        "        subtoken_left = left_side[-1]\n",
        "        # split[1]: 'ly strong' --> split on whitespace ['ly', 'strong']\n",
        "        right_side = subtoken_pieces[1].split()\n",
        "        subtoken_right = right_side[0]\n",
        "        # try merging the subtoken parts to form a token, i.e. moderate + ly\n",
        "        token = \"\".join([subtoken_left, subtoken_right])\n",
        "\n",
        "        if token in token_exceptions:\n",
        "            # if you match an exception, replace the token with the exception\n",
        "            token = token_exceptions[token]\n",
        "\n",
        "        # assume there are no tokens on left and right side of the subtokens' pieces\n",
        "        left_token = None  # if token is at the beginnig\n",
        "        right_token = None  # if token is at the end\n",
        "        # try looking for them\n",
        "        if len(left_side) > 1:\n",
        "            left_token = left_side[-2]\n",
        "        if len(right_side) > 1:\n",
        "            right_token = right_side[1]\n",
        "\n",
        "        # start from a complete match, and progressively remove left and right\n",
        "        # tokens to counter TRADE preprocessing of some tokens\n",
        "        # The order is\n",
        "        # 1. True, True\n",
        "        # 2. True, False\n",
        "        # 3. False, True\n",
        "        # 4. False, False\n",
        "        # basically, at the end you try looking only for the merged token\n",
        "        pattern: str = \"\"\n",
        "        idx: int = -1\n",
        "        for use_left, use_right in product((True, False), (True, False)):\n",
        "            pattern = token\n",
        "            if (left_token is not None) and use_left:\n",
        "                pattern = \" \".join([left_token, pattern])\n",
        "            if right_token is not None and use_right:\n",
        "                pattern = \" \".join([pattern, right_token])\n",
        "\n",
        "            # check if the pattern is in the exceptions\n",
        "            if pattern in pattern_exceptions:\n",
        "                pattern = pattern_exceptions[pattern]\n",
        "            # Search the pattern\n",
        "            idx = original_seq[search_after:].lower().find(pattern)\n",
        "            if idx > -1:\n",
        "                break\n",
        "\n",
        "        error: str = f\"\"\"\n",
        "            Pattern search failed in the following case:\n",
        "            PATTERN =  \\t{pattern}\n",
        "            LEFT SIDE = \\t{left_side}\n",
        "            RIGHT SIDE = \\t{right_side}\n",
        "            ORIG SEQ = \\t{original_seq[search_after:]}\n",
        "\n",
        "            This may be due to further TRADE pre-processing, or not correct merging operation.\n",
        "            To solve this, add a special rule for the token that breaks the code either as a\n",
        "            token_exception or a pattern_exception.\n",
        "        \"\"\"\n",
        "\n",
        "        assert idx > -1, error\n",
        "        # move the index to avoid perfect matches with the same token\n",
        "        # TODO is probably better to move it of len(left_token + token) or\n",
        "        # len(token) depending on the match\n",
        "        search_after += idx + 1\n",
        "        # reconstruct the sentence with the matched pattern\n",
        "        trade_seq = \" \".join([*left_side[:-1], token, *right_side[1:]])\n",
        "\n",
        "        # try splitting the sentence again and repeat the process\n",
        "        subtoken_pieces = re.split(regex, trade_seq, maxsplit=1)\n",
        "    # Good, no subtokens found: return trade seq\n",
        "    return trade_seq\n",
        "\n",
        "\n",
        "def get_json_object(data_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    A function to read a json object and return the python\n",
        "    dictionary associated to it.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    data_path: str\n",
        "        Path to a json file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loaded_json: dict\n",
        "        A loaded json object.\n",
        "    \"\"\"\n",
        "    with open(data_path, \"r\") as data_file:\n",
        "        data = json.load(data_file)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_dialogues(\n",
        "    data_path: str,\n",
        "    data_split: List[str],\n",
        "    replacements: List[Tuple[str, str]],\n",
        ") -> List[List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Load dialogues from data_path, apply trade pre-processing, revert the\n",
        "    subtokenization, and create a dictionary containing the dialogue id,\n",
        "    the turn id, and the corrected sequence.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    data_path: str\n",
        "        Path to the json file containing the data.\n",
        "    data_split: list of str\n",
        "        List of string containing MultiWOZ 2.1 keys of the dialogues\n",
        "        associated to a certain split (train, dev, test).\n",
        "    replacements_path: str\n",
        "        File containing (from, to) pairs, one per line.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dialogues: list of list of dict, keys are str, values could be anything\n",
        "        List of dialogues. Each dialogue is a list of turns. Each turn is a\n",
        "        dict containing dialogue_idx, turn_idx, and the corrected sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_preprocessed_seq(\n",
        "        original_seq: str, replacements: List[Tuple[str, str]]\n",
        "    ) -> str:\n",
        "        # apply trade normalization\n",
        "        trade_seq = normalize(original_seq, replacements)\n",
        "        # merge back subtokens\n",
        "        sequence = invert_trade_subtokenization(original_seq, trade_seq)\n",
        "        return sequence\n",
        "\n",
        "    dialogues: List[List[Dict[str, Any]]] = []\n",
        "\n",
        "    data = get_json_object(data_path)\n",
        "\n",
        "    for dialogue_idx in tqdm(data_split, desc=\"Load Dialogues\"):\n",
        "        dial: List[Dict[str, Any]] = []\n",
        "        original_dialogue: dict = data[dialogue_idx]\n",
        "        turns: dict = original_dialogue[\"log\"]\n",
        "        for i, turn in enumerate(turns):\n",
        "            sequence = get_preprocessed_seq(turn[\"text\"], replacements)\n",
        "            to_save = {\n",
        "                \"sequence\": sequence,\n",
        "                \"turn_idx\": i,\n",
        "                \"dialogue_idx\": dialogue_idx,\n",
        "            }\n",
        "            dial.append(to_save)\n",
        "        dialogues.append(dial)\n",
        "    return dialogues\n",
        "\n",
        "\n",
        "def create_entry_key(turn: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Creates the entry key for a given entry by considering dialogue id\n",
        "    and turn id for the given turn.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    turn: dict, keys are str, values could be anything\n",
        "        A dict containing, the dialogue id, the turn id, the sequence,\n",
        "        and the mean length.\n",
        "    kwargs: any\n",
        "        Additional arguments for the current function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    key: str\n",
        "        The key for the given turn.\n",
        "    \"\"\"\n",
        "    dialogue_idx = turn[\"dialogue_idx\"]\n",
        "    turn_idx = turn[\"turn_idx\"]\n",
        "    return f\"{dialogue_idx}_{turn_idx}\"\n",
        "\n",
        "\n",
        "def create_dialogue_dataset(\n",
        "    dialogues: List[List[Dict[str, Any]]]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Creates a dialogue dataset starting from a set of dialogues. Each\n",
        "    entry of the dataset contains the dialogue history and the system\n",
        "    reply in response to that.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    dialogues: list of list of dict, keys are str, values could be anything\n",
        "        List of dialogues. Each dialogue is a list of turns. Each turn is a\n",
        "        dict containing dialogue_idx, turn_idx, and the corrected sequence.\n",
        "    kwargs: any\n",
        "        Additional arguments for the current function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dataset: Dict[str, Dict[str, Any]]\n",
        "        Dataset, keys are str, values are dictionaries containing the\n",
        "        dialogue history and the system reply.\n",
        "    \"\"\"\n",
        "\n",
        "    def create_dialogue_dataset_entry(\n",
        "        turn: Dict[str, Any], history: List[str]\n",
        "    ) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Creates an entry if the current turn id is odd. An entry is\n",
        "        composed of the history, which contains the previous turns\n",
        "        of the current dialogue, and the reply of the system.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        turn: dict, keys are str, values could be anything\n",
        "            A dict containing, the dialogue id, the turn id, the sequence,\n",
        "            and the mean length.\n",
        "        replacements_path: str\n",
        "            Path to TRADE file containing (from, to) pairs, one per line.\n",
        "        kwargs: any\n",
        "            Additional arguments for the current function.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        entry: optional dict, keys are str, values could be anything\n",
        "            Entry of the dialogue dataset. It is a dict containing the history\n",
        "            of the dialogue, i.e. a list of turns, the reply of the system,\n",
        "            i.e. a turn, and the mean length.\n",
        "        \"\"\"\n",
        "\n",
        "        turn_idx = turn[\"turn_idx\"]\n",
        "        entry: Optional[Dict[str, Any]] = None\n",
        "        if turn_idx % 2 == 0:\n",
        "            # user turn, simply append it to the history\n",
        "            user_seq: str = turn[\"sequence\"]\n",
        "            history.append(user_seq)\n",
        "        elif turn_idx % 2 == 1:\n",
        "            # system turn, create the dataset entry, and the append it to the history\n",
        "            system_seq: str = turn[\"sequence\"]\n",
        "            history_mean_length = mean([len(turn) for turn in history])\n",
        "            entry = {\n",
        "                \"history\": history.copy(),\n",
        "                \"reply\": system_seq,\n",
        "                \"length\": history_mean_length + len(system_seq),\n",
        "            }\n",
        "            history.append(system_seq)\n",
        "        return entry\n",
        "\n",
        "    dataset: Dict[str, Dict[str, Any]] = {}\n",
        "    for dialogue in tqdm(dialogues, desc=\"Creating dataset\"):\n",
        "        history: List[str] = []\n",
        "        for turn in dialogue:\n",
        "            # custom function to create a dataset entry\n",
        "            dataset_entry = create_dialogue_dataset_entry(turn, history)\n",
        "            # custom function to create a dataset key\n",
        "            key = create_entry_key(turn)\n",
        "            if dataset_entry is not None:\n",
        "                dataset[key] = dataset_entry\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def save_dialogue_dataset(\n",
        "    dataset: Dict[str, Dict[str, Any]], file_name: str, dst_folder: str = \".\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Saves the dialogue dataset at dst_folder/file_name as a json file.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    dataset: Dict[str, Dict[str, Any]]\n",
        "        Dataset, keys are str, values are dictionaries containing the\n",
        "        dialogue history, the system reply, and the mean length.\n",
        "    file_name: str\n",
        "        Name of the file where the dataset will be saved.\n",
        "    dst_folder: str\n",
        "        Path to the directory where the dataset will be saved. If it\n",
        "        does not exists, it creates it.\n",
        "    \"\"\"\n",
        "    os.makedirs(dst_folder, exist_ok=True)\n",
        "    dataset_path = os.path.join(dst_folder, file_name)\n",
        "    with open(dataset_path, \"w\") as f:\n",
        "        json.dump(dataset, f, indent=4)\n",
        "\n",
        "\n",
        "def encode_dialogue_dataset(\n",
        "    file_name: str,\n",
        "    dst_folder: str,\n",
        "    data_path: str,\n",
        "    data_split: List[str],\n",
        "    override: bool,\n",
        "    logger: logging.Logger,\n",
        "    replacements_path: str = \"utils/mapping.pair\",\n",
        "    random_dialogues: Optional[int] = None,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Wrapper function that loads processed data stored at\n",
        "    dst_folder/file_name. If they are not available, it processes the\n",
        "    original data and then saves them at dst_folder/file_name.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    file_name: str\n",
        "        Name of the file where the dataset will be saved.\n",
        "    dst_folder: str\n",
        "        Path to the directory where the dataset will be saved. If it\n",
        "        does not exists, it creates it.\n",
        "    data_path: str\n",
        "        Path to the data pre-processed by trade.\n",
        "    data_split: list of str\n",
        "        List of string containing MultiWOZ 2.1 keys of the dialogues\n",
        "        associated to a certain split (train, dev, test).\n",
        "    override: bool\n",
        "        Whether or not override the data stored at dst_folder/file_name.\n",
        "    logger: logging.Logger instance\n",
        "        Logger to report the processing steps carried out in the current\n",
        "        execution.\n",
        "    replacements_path: str\n",
        "        Path to TRADE file containing (from, to) pairs, one per line.\n",
        "    random_dialogues: int\n",
        "        Number of dialogues to randomly sample from the current data.\n",
        "    \"\"\"\n",
        "    dataset_path = os.path.join(dst_folder, file_name)\n",
        "    if os.path.isfile(dataset_path) and (not override):\n",
        "        logger.info(f\"Dataset already created at {dataset_path}\")\n",
        "    else:\n",
        "        replacements = get_replacements(replacements_path)\n",
        "        logger.info(f\"Extract dialogues from {data_path}\")\n",
        "        # custom loading function to return the important elements of a dialogue\n",
        "        dialogues = load_dialogues(data_path, data_split, replacements)\n",
        "        if random_dialogues:\n",
        "            dialogues = random.sample(dialogues, min(random_dialogues, len(dialogues)))\n",
        "        logger.info(\"Create dataset\")\n",
        "        dataset = create_dialogue_dataset(dialogues)\n",
        "        logger.info(f\"Save dataset in {dataset_path}\")\n",
        "        save_dialogue_dataset(dataset, file_name, dst_folder)\n",
        "\n",
        "\n",
        "def build_dialogue_dataset(\n",
        "    data_path: str,\n",
        "    logger: logging.Logger,\n",
        "    data_split: List[str],\n",
        "    file_name: str = \"train.json\",\n",
        "    dst_folder: str = \".\",\n",
        "    override: bool = False,\n",
        "    replacements_path: str = \"utils/mapping.pair\",\n",
        "    random_dialogues: Optional[int] = None,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Returns the dialogue dataset for the corresponding data_path.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    data_path: str\n",
        "        Path to the data pre-processed by trade.\n",
        "    logger: logging.Logger instance\n",
        "        Logger to report the processing steps carried out in the current\n",
        "        execution.\n",
        "    data_split: list of str\n",
        "        List of string containing MultiWOZ 2.1 keys of the dialogues\n",
        "        associated to a certain split (train, dev, test).\n",
        "    file_name: str\n",
        "        Name of the file where the dataset will be saved.\n",
        "    dst_folder: str\n",
        "        Path to the directory where the dataset will be saved. If it\n",
        "        does not exists, it creates it.\n",
        "    override: bool\n",
        "        Whether or not override the data stored at dst_folder/file_name.\n",
        "    replacements_path: str\n",
        "        Path to TRADE file containing (from, to) pairs, one per line.\n",
        "    random_dialogues: int\n",
        "        Number of dialogues to randomly sample from the current data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dataset:\n",
        "        dataset, keys are str, values are dictionaries containing the\n",
        "        dialogue history, the system reply, and the mean length.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Prepare {file_name}\")\n",
        "    encode_dialogue_dataset(\n",
        "        file_name,\n",
        "        dst_folder,\n",
        "        data_path,\n",
        "        data_split,\n",
        "        override,\n",
        "        logger,\n",
        "        replacements_path,\n",
        "        random_dialogues=random_dialogues,\n",
        "    )\n",
        "\n",
        "\n",
        "def download_mwoz_21(destination):\n",
        "    \"\"\"Download dataset repo, unpack it, and remove unnecessary elements.\n",
        "    Arguments\n",
        "    ---------\n",
        "    destination : str\n",
        "        Place to put dataset.\n",
        "    \"\"\"\n",
        "    mwoz_21_archive = os.path.join(destination, \"MultiWOZ_21.zip\")\n",
        "    download_file(MULTIWOZ_21_DATASET_URL, mwoz_21_archive)\n",
        "    shutil.unpack_archive(mwoz_21_archive, destination)\n",
        "    shutil.rmtree(os.path.join(destination, \"__MACOSX\"))\n",
        "\n",
        "    mwoz_21 = os.path.join(destination, \"MultiWOZ_21\")\n",
        "    os.makedirs(mwoz_21, exist_ok=True)\n",
        "\n",
        "    mwoz_21_repo = os.path.join(destination, \"MultiWOZ_2.1\")\n",
        "    for relevant_file in [\"data.json\", \"valListFile.txt\", \"testListFile.txt\"]:\n",
        "        shutil.move(\n",
        "            os.path.join(mwoz_21_repo, relevant_file),\n",
        "            os.path.join(mwoz_21, relevant_file),\n",
        "        )\n",
        "\n",
        "    shutil.rmtree(mwoz_21_repo)\n",
        "\n",
        "\n",
        "def get_splits(dataset_folder) -> Tuple[List[str], List[str], List[str]]:\n",
        "    mwoz_21_dialouges = get_json_object(os.path.join(dataset_folder, \"data.json\"))\n",
        "    dialougues_keys: Set[str] = set(mwoz_21_dialouges.keys())\n",
        "    tr_split: List[str] = []\n",
        "    with open(os.path.join(dataset_folder, \"valListFile.txt\")) as f:\n",
        "        dev_split: List[str] = [key.strip() for key in f]\n",
        "    with open(os.path.join(dataset_folder, \"testListFile.txt\")) as f:\n",
        "        te_split: List[str] = [key.strip() for key in f]\n",
        "\n",
        "    for key in dialougues_keys:\n",
        "        if key not in dev_split and key not in te_split:\n",
        "            tr_split.append(key)\n",
        "\n",
        "    return tr_split, dev_split, te_split\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6h18vok49c0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ee145e-7e9f-4c8c-9e62-e15e783d6b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.9/dist-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this tutorial, we only use 1000, 100 and 200 sentences for training, valid and test respectively."
      ],
      "metadata": {
        "id": "Kuok1VOkWES_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_mwoz_21(\"data_dir\", \"data\", False, \"mapping.pair\", 1000, 100, 200)\n"
      ],
      "metadata": {
        "id": "B3Z3EljGDskB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Tokenizer**\n",
        "GPT-2 has its own tokenizer, it is called GPT2Tokenizer. We can load the pre-trained version as we do for the model.\n",
        "\n",
        "```\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(hparams['gpt_hub'])\n",
        "```\n",
        "We need to add special tokens to the tokenizer to identify which speaker is talking (system or user).\n",
        "\n",
        "\n",
        "```\n",
        "def add_special_tokens_(\n",
        "    model: Union[GPT2LMHeadModel, GPT2DoubleHeadsModel],\n",
        "    tokenizer: GPT2Tokenizer,\n",
        "    attr_to_special_token: Dict[str, Union[str, List[str]]],\n",
        ") -> None:\n",
        "    orig_num_tokens = len(tokenizer.encoder)\n",
        "    num_added_tokens = tokenizer.add_special_tokens(\n",
        "        attr_to_special_token  # type: ignore\n",
        "    )  # doesn't add if they are already there\n",
        "    if num_added_tokens > 0:\n",
        "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cP1IrVEtF9m8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Train a Model**"
      ],
      "metadata": {
        "id": "7xsnCbEJIdC9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBqJpUdUi2aL"
      },
      "source": [
        "Since we are performing a language modeling task, for our model, we will be finetuning GPT2LMHeadModel. For fine-tuning, it is enough to load the pre-trained version of the model and use the default forward method implementation. It is important to specify the correct hugging face path, which for this model is gpt_hub: gpt2.\n",
        "\n",
        "The hyperparameter file for our model is the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj8O4cLCGMRX",
        "outputId": "c95b1e5f-c541-468b-947f-822cde723d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hparams_gpt2.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_gpt2.yaml\n",
        "\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
        "seed: 1993\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Dataset will be downloaded to the `data_original`\n",
        "data_folder: output\n",
        "output_folder: !ref /content/results/train_with_gpt2/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# URL for the gpt2 model\n",
        "gpt_hub: gpt2\n",
        "gpt_folder: !ref <save_folder>/gpt_checkpoint\n",
        "\n",
        "# Path where data manifest files will be stored\n",
        "train_annotation: !ref <data_folder>/train.json\n",
        "valid_annotation: !ref <data_folder>/valid.json\n",
        "test_annotation: !ref <data_folder>/test.json\n",
        "\n",
        "# The train logger writes training statistics to a file, as well as stdout.\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "# Special tokens\n",
        "bos_token: \"BOS\"\n",
        "eos_token: \"EOS\"\n",
        "\n",
        "system_token: \"SPK_1\"\n",
        "user_token: \"SPK_2\"\n",
        "\n",
        "additional_special_tokens : [\n",
        "    !ref <system_token>,\n",
        "    !ref <user_token>\n",
        "]\n",
        "\n",
        "special_tokens: [\n",
        "    !ref <bos_token>,\n",
        "    !ref <eos_token>,\n",
        "    !ref <system_token>,\n",
        "    !ref <user_token>\n",
        "]\n",
        "\n",
        "attr_to_special_tokens:\n",
        "    \"bos_token\": !ref <bos_token>\n",
        "    \"eos_token\": !ref <eos_token>\n",
        "    \"additional_special_tokens\": !ref <additional_special_tokens>\n",
        "\n",
        "# Training parameters\n",
        "number_of_epochs: 30\n",
        "batch_size: 8\n",
        "lr: 1.97125e-4\n",
        "max_history: 2\n",
        "with_eos: True\n",
        "\n",
        "#freeze GPT model\n",
        "freeze_gptmodel: False\n",
        "\n",
        "# Model parameters\n",
        "# [TODO] if any\n",
        "\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: True\n",
        "    num_workers: 2  # 2 on linux but 0 works on windows\n",
        "    drop_last: False\n",
        "\n",
        "# gpt model\n",
        "gpt_model: !new:huggingface_GPT.HuggingFaceGPT\n",
        "    source: !ref <gpt_hub>\n",
        "    freeze: !ref <freeze_gptmodel>\n",
        "    save_path: !ref <gpt_folder>\n",
        "\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "modules:\n",
        "    gpt_model: !ref <gpt_model>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <gpt_model>]\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "\n",
        "\n",
        "opt_class: !name:torch.optim.AdamW\n",
        "    lr: !ref <lr>\n",
        "\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        gpt_model: !ref <gpt_model>\n",
        "        lr_annealing_output: !ref <lr_annealing>\n",
        "        counter: !ref <epoch_counter>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYcKmhPmlALt"
      },
      "source": [
        "The training script follows a standard approach, and you should be able to identify the common operations that are necessary to implement a neural classifier:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTjZ1nIkHBW0",
        "outputId": "b2ee6a59-0c45-4dd0-81d6-2beb0a9da331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ],
      "source": [
        "%%file train.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import speechbrain as sb\n",
        "import torch\n",
        "from itertools import chain\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "\n",
        "class ResGenBrain(sb.Brain):\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + emotion classifier.\n",
        "        \"\"\"\n",
        "        # Get required data from batch\n",
        "        batch = batch.to(self.device)\n",
        "        input_ids, _ = batch.input_ids\n",
        "        token_type_ids, _ = batch.token_type_ids\n",
        "\n",
        "        # Forward Pass\n",
        "        outputs = self.modules.gpt_model(\n",
        "            input_ids,\n",
        "            token_type_ids,\n",
        "        ).logits\n",
        "\n",
        "        #  apply softmax if necessary\n",
        "        outputs = self.hparams.log_softmax(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using speaker-id as label.\n",
        "        \"\"\"\n",
        "        # Get required data from batch\n",
        "        lm_labels, labels_lens = batch.lm_labels\n",
        "\n",
        "        # Calculate Loss function\n",
        "        # predictions_flatten = predictions.contiguous().view(-1, predictions.size(-1))\n",
        "        # lm_labels_flatten = lm_labels.contiguous().view(-1)\n",
        "\n",
        "        loss = self.hparams.compute_cost(predictions, lm_labels, labels_lens)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def fit_batch(self, batch):\n",
        "        \"\"\"Trains the parameters given a single batch in input\"\"\"\n",
        "\n",
        "        predictions = self.compute_forward(batch, sb.Stage.TRAIN)\n",
        "        loss = self.compute_objectives(predictions, batch, sb.Stage.TRAIN)\n",
        "        loss.backward()\n",
        "        if self.check_gradients(loss):\n",
        "            self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        return loss.detach()\n",
        "\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch):\n",
        "        \"\"\"Gets called at the end of an epoch.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
        "        stage_loss : float\n",
        "            The average loss for all of the data processed in this stage.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "\n",
        "        # Store the train loss until the validation stage.\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        elif stage == sb.Stage.VALID:\n",
        "\n",
        "\n",
        "            # Update learning rate\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            # The train_logger writes a summary to stdout and to the logfile.\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats={\n",
        "                    \"loss\": stage_loss,\n",
        "                },\n",
        "            )\n",
        "            # Save the current checkpoint and delete previous checkpoints.\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"loss\": stage_stats[\"loss\"]}, min_keys=[\"loss\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        elif stage == sb.Stage.TEST:\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats={\n",
        "                    \"loss\": stage_loss,\n",
        "                },\n",
        "            )\n",
        "\n",
        "    def init_optimizers(self):\n",
        "        \"Initializes the model optimizer\"\n",
        "        self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "        if self.checkpointer is not None:\n",
        "            self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "    def zero_grad(self, set_to_none=False):\n",
        "        self.optimizer.zero_grad(set_to_none)\n",
        "\n",
        "\n",
        "def add_special_tokens_(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    attr_to_special_token,\n",
        ") -> None:\n",
        "    orig_num_tokens = len(tokenizer.encoder)\n",
        "    num_added_tokens = tokenizer.add_special_tokens(\n",
        "        attr_to_special_token  # type: ignore\n",
        "    )  # doesn't add if they are already there\n",
        "    if num_added_tokens > 0:\n",
        "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
        "\n",
        "\n",
        "def dataio_prep(hparams, tokenizer):\n",
        "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
        "    It also defines the data processing pipeline through user-defined\n",
        "    functions. We expect `prepare_mini_librispeech` to have been called before\n",
        "    this, so that the `train.json`, `valid.json`,  and `valid.json` manifest\n",
        "    files are available.\n",
        "    Arguments\n",
        "    ---------\n",
        "    hparams : dict\n",
        "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
        "        all the hyperparameters needed for dataset construction and loading.\n",
        "    Returns\n",
        "    -------\n",
        "    datasets : dict\n",
        "        Contains two keys, \"train\" and \"valid\" that correspond\n",
        "        to the appropriate DynamicItemDataset object.\n",
        "    \"\"\"\n",
        "\n",
        "    # convert special tokens to their ids\n",
        "    bos, eos, system, user =  tokenizer.convert_tokens_to_ids(hparams[\"special_tokens\"])\n",
        "    # history_window, i.e. how many user-system exchanges consider as context (+1 to consider at least the last user turn)\n",
        "    history_window = 2*hparams[\"max_history\"]+1\n",
        "\n",
        "    #  Define histoy pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"history\")\n",
        "    @sb.utils.data_pipeline.provides(\n",
        "        \"history\", \"history_tokens_lists\", \"history_input_lists\", \"history_token_type_lists\"\n",
        "    )\n",
        "    def history_pipeline(history):\n",
        "        yield history\n",
        "\n",
        "        # encode each turn of the history\n",
        "        history_tokens_lists = [tokenizer.encode(turn) for turn in history]\n",
        "        yield history_tokens_lists\n",
        "\n",
        "        # add speaker tokens to the history turns (user is even, system is odd)\n",
        "        # BEFORE:  [Hi how are you?], [I'm fine, thanks]\n",
        "        # AFTER:   [SPK_1 Hi how are you?], [SPK_2 I'm fine, thanks]\n",
        "        history_input_lists = [[user if i%2==0 else system] + encoded_turn for i, encoded_turn in enumerate(history_tokens_lists)]\n",
        "        yield history_input_lists\n",
        "\n",
        "        # create a mapping that associates each token in the input to a speaker\n",
        "        # INPUT: [SPK_1 Hi    how   are   you? ], [SPK_2 I'm   fine, thanks]\n",
        "        # TYPE:  [SPK_1 SPK_1 SPK_1 SPK_1 SPK_1], [SPK_2 SPK_2 SPK_2 SPK_2 ]\n",
        "        history_token_type_lists = [[user if i%2==0 else system]*len(encoded_turn) for i, encoded_turn in enumerate(history_input_lists)]\n",
        "        yield history_token_type_lists\n",
        "\n",
        "\n",
        "    #  Define reply pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"reply\")\n",
        "    @sb.utils.data_pipeline.provides(\n",
        "        \"reply\", \"reply_tokens_list\", \"reply_input_list\", \"reply_token_type_list\"\n",
        "    )\n",
        "    def reply_pipeline(reply):\n",
        "        yield reply\n",
        "\n",
        "        # same as history\n",
        "        reply_tokens_list = tokenizer.encode(reply)\n",
        "        yield reply_tokens_list\n",
        "\n",
        "        # specify that the system will say the reply\n",
        "        reply_input_list = [system] + reply_tokens_list\n",
        "        yield reply_input_list\n",
        "\n",
        "        # specify the speaker for each token in the reply\n",
        "        reply_token_type_list = [system]*len(reply_input_list)\n",
        "        yield reply_token_type_list\n",
        "\n",
        "\n",
        "    # Define input_and_token_type_pipeline\n",
        "    @sb.utils.data_pipeline.takes(\n",
        "        \"history_input_lists\", \"history_token_type_lists\", \"reply_input_list\", \"reply_token_type_list\"\n",
        "    )\n",
        "    @sb.utils.data_pipeline.provides(\"input_ids\", \"token_type_ids\", \"lm_labels\")\n",
        "    def input_and_token_type_pipeline(history_input_lists, history_token_type_lists, reply_input_list, reply_token_type_list):\n",
        "        # optionally add eos to reply\n",
        "        reply_input_list = reply_input_list + [eos] if hparams[\"with_eos\"] else []\n",
        "\n",
        "        # add bos and to the history\n",
        "        history_input_lists = [[bos]] + history_input_lists[-history_window:]\n",
        "\n",
        "        # put history and reply together\n",
        "        input_sequence = history_input_lists + [reply_input_list]\n",
        "\n",
        "        # concatenate every token into a single list\n",
        "        # list(chain(*[[1, 2], [3, 4], [5]]))\n",
        "        # >>> [1, 2, 3, 4, 5]\n",
        "        input_ids = list(chain(*input_sequence))\n",
        "        input_ids = torch.LongTensor(input_ids)\n",
        "        yield input_ids\n",
        "\n",
        "        # do the same for the token_type\n",
        "        reply_token_type_list = reply_token_type_list + [system] if hparams[\"with_eos\"] else []\n",
        "\n",
        "        # bos token belongs to the system\n",
        "        history_token_type_lists = [[system]] + history_token_type_lists[-history_window:]\n",
        "\n",
        "        token_type_ids = history_token_type_lists + [reply_token_type_list]\n",
        "\n",
        "        token_type_ids = list(chain(*token_type_ids))\n",
        "        token_type_ids = torch.LongTensor(token_type_ids)\n",
        "        yield token_type_ids\n",
        "\n",
        "        # create the language model label (ground truth) for the current input\n",
        "        # -100 is a special tokens that is ignored during the loss computation\n",
        "        # the idea is to mask everything except the reply (withouth the speaker token)\n",
        "        lm_labels = ([-100] * sum(len(s) for s in input_sequence[:-1])) + [-100] + input_sequence[-1][1:]\n",
        "        lm_labels = torch.LongTensor(lm_labels)\n",
        "        yield lm_labels\n",
        "\n",
        "\n",
        "    # Define datasets. We also connect the dataset with the data processing\n",
        "    # functions defined above.\n",
        "    datasets = {}\n",
        "    data_info = {\n",
        "        \"train\": hparams[\"train_annotation\"],\n",
        "        \"valid\": hparams[\"valid_annotation\"],\n",
        "        \"test\": hparams[\"test_annotation\"],\n",
        "    }\n",
        "    for dataset in data_info:\n",
        "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
        "            json_path=data_info[dataset],\n",
        "            replacements={\"data_root\": hparams[\"data_folder\"]},\n",
        "            dynamic_items=[reply_pipeline, history_pipeline, input_and_token_type_pipeline],\n",
        "            output_keys=[\"id\", \"input_ids\", \"token_type_ids\", \"lm_labels\"],\n",
        "        )\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# RECIPE BEGINS!\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Reading command line arguments.\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training).\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides.\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Load tokenizer and add special tokens\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(hparams['gpt_hub'])\n",
        "\n",
        "    #  Load pretrained GPT\n",
        "    hparams[\"gpt_model\"] = hparams[\"gpt_model\"].to(device=run_opts[\"device\"])\n",
        "\n",
        "    # Add special tokens to the tokenizer and resize model embedding\n",
        "    add_special_tokens_(hparams[\"gpt_model\"].model, tokenizer, hparams[\"attr_to_special_tokens\"])\n",
        "\n",
        "    # Create dataset objects \"train\", \"valid\", and \"test\".\n",
        "    datasets = dataio_prep(hparams, tokenizer)\n",
        "\n",
        "    # Initialize the Brain object to prepare for mask training.\n",
        "    res_gen_brain = ResGenBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # The `fit()` method iterates the training loop, calling the methods\n",
        "    # necessary to update the parameters of the model. Since all objects\n",
        "    # with changing state are managed by the Checkpointer, training can be\n",
        "    # stopped at any point, and will be resumed on next call.\n",
        "    res_gen_brain.fit(\n",
        "        epoch_counter=res_gen_brain.hparams.epoch_counter,\n",
        "        train_set=datasets[\"train\"],\n",
        "        valid_set=datasets[\"valid\"],\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = res_gen_brain.evaluate(\n",
        "        test_set=datasets[\"test\"],\n",
        "        min_key=\"error_rate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx3p1jWfHo_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc2caf5-7acc-48c5-bb38-43f3a93dd125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.9/dist-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "Downloading (…)lve/main/config.json: 100% 665/665 [00:00<00:00, 96.2kB/s]\n",
            "Downloading pytorch_model.bin: 100% 548M/548M [00:01<00:00, 289MB/s]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 20.0kB/s]\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: /content/results/train_with_gpt2/1993\n",
            "speechbrain.core - 38.6M trainable parameters in ResGenBrain\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100% 840/840 [03:55<00:00,  3.57it/s, train_loss=4.39]\n",
            "100% 93/93 [00:10<00:00,  8.51it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.97e-04 - train loss: 4.39 - valid loss: 2.20e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /content/results/train_with_gpt2/1993/save/CKPT+2023-04-04+19-22-45+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100% 840/840 [03:59<00:00,  3.50it/s, train_loss=0.21]\n",
            "100% 93/93 [00:10<00:00,  8.65it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0002 to 0.00018\n",
            "speechbrain.utils.train_logger - epoch: 2, lr: 1.97e-04 - train loss: 2.10e-01 - valid loss: 4.35e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /content/results/train_with_gpt2/1993/save/CKPT+2023-04-04+19-26-59+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in /content/results/train_with_gpt2/1993/save/CKPT+2023-04-04+19-22-45+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100% 840/840 [03:57<00:00,  3.54it/s, train_loss=0.0783]\n",
            "100% 93/93 [00:10<00:00,  8.60it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00018 to 0.00016\n",
            "speechbrain.utils.train_logger - epoch: 3, lr: 1.77e-04 - train loss: 7.83e-02 - valid loss: 2.20e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /content/results/train_with_gpt2/1993/save/CKPT+2023-04-04+19-31-11+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in /content/results/train_with_gpt2/1993/save/CKPT+2023-04-04+19-26-59+00\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "100% 184/184 [00:21<00:00,  8.55it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 3 - test loss: 2.07e-02\n"
          ]
        }
      ],
      "source": [
        "!rm -rf results\n",
        "!python train.py hparams_gpt2.yaml --data_folder='/content/data_dir' --device='cuda:0' --number_of_epochs=3 --batch_size=8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjFo7OrLSUjy"
      },
      "source": [
        "\n",
        "\n",
        "Training the model for 3 epochs and only on 1000 data, we could get a test loss of 2.25e-02. For this tutorial, we don't have any beam_search for generating the response during validation and test without teacher_forcing since they could be resource-demanding. But in a real experiment, we need to have a beam searcher that could explore the most probable responses. We also need to report  WER and CER. We will do that in our lab assignment.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce8ecaca819946efa0ac9465b9d286b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49078ad2bc544d6dae3721d42db4e799",
              "IPY_MODEL_956383078d1a4dc6884224e9a3aa6fab",
              "IPY_MODEL_f4ebd9dc90a04fcdb238d1ab2161259c"
            ],
            "layout": "IPY_MODEL_bcb1fae90575471f93203330e0367f7a"
          }
        },
        "49078ad2bc544d6dae3721d42db4e799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_693f3bbebd934047b1ed9d552ed600c1",
            "placeholder": "​",
            "style": "IPY_MODEL_7bc1a52ce9114ef5aed15f4e521e8a45",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "956383078d1a4dc6884224e9a3aa6fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be7f6f18ace04eb9aace3e31dc2b1167",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1929deabceb4defad6c1a259d24468e",
            "value": 665
          }
        },
        "f4ebd9dc90a04fcdb238d1ab2161259c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e706f0e4f11b45bd92794784bb5b3d26",
            "placeholder": "​",
            "style": "IPY_MODEL_2fa8f6f4d23c425c8a27c3465a8e10f0",
            "value": " 665/665 [00:00&lt;00:00, 15.2kB/s]"
          }
        },
        "bcb1fae90575471f93203330e0367f7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "693f3bbebd934047b1ed9d552ed600c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bc1a52ce9114ef5aed15f4e521e8a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be7f6f18ace04eb9aace3e31dc2b1167": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1929deabceb4defad6c1a259d24468e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e706f0e4f11b45bd92794784bb5b3d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fa8f6f4d23c425c8a27c3465a8e10f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "875725309be04a70b6f025e736dff5f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e07b4d205694e308b105645d74117fc",
              "IPY_MODEL_b1a3f7079a7c402cbe8244493182bdd8",
              "IPY_MODEL_847f4a14a91f4464833c588161ca5244"
            ],
            "layout": "IPY_MODEL_07fc7eb76d5a467e914bdc566e6ecd35"
          }
        },
        "3e07b4d205694e308b105645d74117fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59bd8095e86f44ee9c8817019be6b1c5",
            "placeholder": "​",
            "style": "IPY_MODEL_fef6daf649394c2cbbdc6afcaced9b98",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "b1a3f7079a7c402cbe8244493182bdd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fbe4239e38f40e0beda29d545dc10bf",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bd2b388ed9640f8b1d9755f97201f4d",
            "value": 548118077
          }
        },
        "847f4a14a91f4464833c588161ca5244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1810dbe227f846d9bf0e692e90adc7ea",
            "placeholder": "​",
            "style": "IPY_MODEL_0b0a37786fb04495913c99003bc9cbde",
            "value": " 548M/548M [00:02&lt;00:00, 205MB/s]"
          }
        },
        "07fc7eb76d5a467e914bdc566e6ecd35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59bd8095e86f44ee9c8817019be6b1c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fef6daf649394c2cbbdc6afcaced9b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fbe4239e38f40e0beda29d545dc10bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd2b388ed9640f8b1d9755f97201f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1810dbe227f846d9bf0e692e90adc7ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b0a37786fb04495913c99003bc9cbde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e3b538431294564a982053fdf691d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a03750595da463d8066363c693a68d6",
              "IPY_MODEL_632eec3684f64d22805afa3fbddaf590",
              "IPY_MODEL_7e0858de68574b50bc477033fc761451"
            ],
            "layout": "IPY_MODEL_e6105e27ee834a4395be7a17ad2162ac"
          }
        },
        "9a03750595da463d8066363c693a68d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08d3ddeca7d740d3b5ad15063d003a36",
            "placeholder": "​",
            "style": "IPY_MODEL_c36d4821b70840888cc31d7cbb433e4a",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "632eec3684f64d22805afa3fbddaf590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33c2c3f0725147b7b807a5e4915aa5d5",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5facd634842444e8297d232335cab46",
            "value": 124
          }
        },
        "7e0858de68574b50bc477033fc761451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee18511d180440a18386adba30df1b8e",
            "placeholder": "​",
            "style": "IPY_MODEL_14ba8a720e5a46198c2e3c40271e4a54",
            "value": " 124/124 [00:00&lt;00:00, 2.17kB/s]"
          }
        },
        "e6105e27ee834a4395be7a17ad2162ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08d3ddeca7d740d3b5ad15063d003a36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c36d4821b70840888cc31d7cbb433e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33c2c3f0725147b7b807a5e4915aa5d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5facd634842444e8297d232335cab46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee18511d180440a18386adba30df1b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14ba8a720e5a46198c2e3c40271e4a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "687628988b704f13bab9fac9a9689ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b39335dcf444009950a9f06e4702601",
              "IPY_MODEL_b20e41ac654543138e0b09e6646e7aea",
              "IPY_MODEL_aebfb624015545b5af04f3a396cdd11b"
            ],
            "layout": "IPY_MODEL_8f8704087f644f968eb95a320dbc0d1a"
          }
        },
        "0b39335dcf444009950a9f06e4702601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c52b02bc2bb464ebb88af8708fb4edf",
            "placeholder": "​",
            "style": "IPY_MODEL_9656595fcf4941868c9b2f282440a34d",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "b20e41ac654543138e0b09e6646e7aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04e24e29af1544abab400be534d371a0",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9dc64c0e9864647a7e9dac8c1c6f35a",
            "value": 1042301
          }
        },
        "aebfb624015545b5af04f3a396cdd11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f3c7742c2684058b646e321778123dc",
            "placeholder": "​",
            "style": "IPY_MODEL_67a4021781d34bb18f71108334e2fe6f",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 4.98MB/s]"
          }
        },
        "8f8704087f644f968eb95a320dbc0d1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c52b02bc2bb464ebb88af8708fb4edf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9656595fcf4941868c9b2f282440a34d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04e24e29af1544abab400be534d371a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9dc64c0e9864647a7e9dac8c1c6f35a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f3c7742c2684058b646e321778123dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67a4021781d34bb18f71108334e2fe6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b770c7693b52474f9b0fdd1f119488fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa4915d2205d4b20ab9806bf54efb282",
              "IPY_MODEL_e97c8d147622417dad88874890eb887d",
              "IPY_MODEL_d522e6ed913b4a98920042a5c74117b0"
            ],
            "layout": "IPY_MODEL_3c4bb66b670a4ca99677826b3df208a2"
          }
        },
        "fa4915d2205d4b20ab9806bf54efb282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d5f12c1b6fb4301b0722ba589c6885c",
            "placeholder": "​",
            "style": "IPY_MODEL_78586f3a880f445493444e263b2384b5",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "e97c8d147622417dad88874890eb887d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5fe559c3bfe43bf9653fe6ea14f6436",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0793951f79b48c1a37f3740224e2872",
            "value": 456318
          }
        },
        "d522e6ed913b4a98920042a5c74117b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bee5bddcbfc4fefaddf3a230a0b9a00",
            "placeholder": "​",
            "style": "IPY_MODEL_d2a9a8c9b4ef41e5b79073d8729e9df2",
            "value": " 456k/456k [00:00&lt;00:00, 6.71MB/s]"
          }
        },
        "3c4bb66b670a4ca99677826b3df208a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d5f12c1b6fb4301b0722ba589c6885c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78586f3a880f445493444e263b2384b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5fe559c3bfe43bf9653fe6ea14f6436": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0793951f79b48c1a37f3740224e2872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3bee5bddcbfc4fefaddf3a230a0b9a00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2a9a8c9b4ef41e5b79073d8729e9df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9329c9e521f4412bf0e6b4bc889724a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a53fb1b37e0c4a2198601624656e8d4a",
              "IPY_MODEL_e17d906d143442f192e6a8aa031f7403",
              "IPY_MODEL_3f8cfa75ed6a40458845dc5b4f6d9ca8"
            ],
            "layout": "IPY_MODEL_02fa4e4c6c8046d182e4798aca67c1fe"
          }
        },
        "a53fb1b37e0c4a2198601624656e8d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b86e460d3db4293b7ba4d13570aa3b4",
            "placeholder": "​",
            "style": "IPY_MODEL_faf66d5252d645959b594e0576af4ff9",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "e17d906d143442f192e6a8aa031f7403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b8b2285ed1f4e80bb95345a9c1c8fbe",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10f822100a564d4fb75284a91d70e509",
            "value": 1355256
          }
        },
        "3f8cfa75ed6a40458845dc5b4f6d9ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51910981257a4fccba669e5b8e96b571",
            "placeholder": "​",
            "style": "IPY_MODEL_0977720d918e4df7b79a310ff05a942e",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 6.37MB/s]"
          }
        },
        "02fa4e4c6c8046d182e4798aca67c1fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b86e460d3db4293b7ba4d13570aa3b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faf66d5252d645959b594e0576af4ff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b8b2285ed1f4e80bb95345a9c1c8fbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10f822100a564d4fb75284a91d70e509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51910981257a4fccba669e5b8e96b571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0977720d918e4df7b79a310ff05a942e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}