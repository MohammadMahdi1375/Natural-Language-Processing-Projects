{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import spacy\n",
    "###### spaCy has model for each language (\"de_core_news_sm\" for German and \"en_core_web_sm\" for English) which need to be loaded so we can access the tokenizer of each model\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer\n",
    "from datasets import load_metric\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "BATCH_SIZE = 32\n",
    "random.seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEVICE ='cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1) Text Preprocessing and Bucketing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Dutch Vocabulary: 7344\n",
      "Size of English Vocabulary: 5641\n"
     ]
    }
   ],
   "source": [
    "trg_Field = torchtext.data.Field(tokenize=lambda text: [token.text for token in spacy_en(text)], eos_token=\"<eos>\", init_token=\"<bos>\", lower=True, include_lengths=True, batch_first=True)\n",
    "src_Field = torchtext.data.Field(tokenize=lambda text: [token.text for token in spacy_de(text)], eos_token=\"<eos>\", init_token=\"<bos>\", lower=True, include_lengths=True, batch_first=True)\n",
    "\n",
    "fields = [(\"src_field\", src_Field), (\"trg_field\", trg_Field)]\n",
    "\n",
    "\n",
    "if (not os.path.exists(\"data_de_en.csv\")):\n",
    "    train_en_data = pd.read_csv(\"./multi30k-dataset/data/task1/raw/train.en\", delimiter='\\t', header=None)\n",
    "    train_de_data = pd.read_csv(\"./multi30k-dataset/data/task1/raw/train.de\", delimiter='\\t', header=None)\n",
    "\n",
    "    df = pd.concat([train_de_data, train_en_data], axis=1)\n",
    "\n",
    "    df.to_csv(\"data_de_en.csv\")\n",
    "\n",
    "TabularData = torchtext.data.TabularDataset(path=\"data_de_en.csv\",\n",
    "                                            format=\"CSV\",\n",
    "                                            fields = fields,\n",
    "                                            skip_header=True)\n",
    "train_data, valid_data, test_data = TabularData.split(split_ratio=[0.9, 0.05, 0.05], random_state=random.seed(SEED))\n",
    "\n",
    "\n",
    "## Bucket Iterator\n",
    "train_Iterator = torchtext.data.BucketIterator(dataset=train_data,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               device=DEVICE,\n",
    "                                               sort_key=lambda x: len(x.src_field),\n",
    "                                               sort_within_batch=True)\n",
    "test_Iterator = torchtext.data.BucketIterator(dataset=test_data,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              device=DEVICE,\n",
    "                                              sort_key=lambda x: len(x.src_field),\n",
    "                                              sort_within_batch=True)\n",
    "valid_Iterator = torchtext.data.BucketIterator(dataset=valid_data,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               sort_key=lambda x: len(x.src_field),\n",
    "                                               sort_within_batch= True)\n",
    "\n",
    "## Building Vocabulary\n",
    "src_Field.build_vocab(train_data, min_freq=2)\n",
    "trg_Field.build_vocab(train_data, min_freq=2)\n",
    "print(f\"Size of Dutch Vocabulary: {len(src_Field.vocab)}\")\n",
    "print(f\"Size of English Vocabulary: {len(trg_Field.vocab)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2) Positional Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        ##### position shape: (max_seq_length, 1)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        ##### position shape: (d_model/2, )\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "                \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3) Multi-Head Attention Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n",
      "tensor([[[ 7.2706,  7.1466,  8.4439, -8.3005],\n",
      "         [ 2.5720,  2.5876,  2.7680, -2.5002],\n",
      "         [ 4.9907,  4.8779,  5.7770, -5.5623]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[0.3333, 0.3333, 0.3333],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.3333, 0.3333, 0.3333]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "\n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.hid_dim // self.n_heads\n",
    "\n",
    "        self.fc_q = torch.nn.Linear(self.hid_dim, self.hid_dim)\n",
    "        self.fc_k = torch.nn.Linear(self.hid_dim, self.hid_dim)\n",
    "        self.fc_v = torch.nn.Linear(self.hid_dim, self.hid_dim)\n",
    "\n",
    "        self.fc_o = torch.nn.Linear(self.hid_dim, self.hid_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        ##### query: (batch_size, seq_len, hid_dim)\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        ##### Q: (batch_size, seq_len, hid_dim)\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_q(key)\n",
    "        V = self.fc_q(value)\n",
    "\n",
    "        ##### Q: (batch_size, n_heads, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        ##### Unnormalized Attention Weights: (batch_size, n_heads, query_len, key_len)  ---- query_len = seq_len, key_len = seq_len\n",
    "        energy = torch.matmul(Q, K.permute(0,1, 3, 2)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = self.softmax(energy)\n",
    "\n",
    "        ##### Attention: (batch_size, n_heads, query_len, head_dim)\n",
    "        x = torch.matmul(self.dropout(energy), V)\n",
    "\n",
    "        ##### Attention: (batch_size, query_len, n_heads, head_dim)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        ##### size: (batch_size, seq_len, hid_dim)\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "\n",
    "        ##### size: (batch_size, seq_len, hid_dim)\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "att = MultiHeadAttentionLayer(4, 2, 0.5, DEVICE).to(DEVICE)\n",
    "src = torch.ones((1,3,4)).to(DEVICE)\n",
    "print(src.shape)\n",
    "\n",
    "a, b = att(src, src, src)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4) Position-wise Feedforward Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(torch.nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super(PositionwiseFeedforwardLayer, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = torch.nn.Linear(pf_dim, hid_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5) Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_hid, dropout, device):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_ff = PositionwiseFeedforwardLayer(hid_dim, pf_hid, dropout)\n",
    "        self.atten_layre_norm = torch.nn.LayerNorm(hid_dim)\n",
    "        self.pw_ff_layer_norm = torch.nn.LayerNorm(hid_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        ##### src = (batch_size, src_len, hid_dim)\n",
    "        ##### src_mask = (batch_size, 1, 1, src_len)\n",
    "        ##### att_out: (batch_size, seq_len, hid_dim)\n",
    "        att_out, _ = self.self_attention(src, src, src, src_mask)\n",
    "        pw_ff_in = self.atten_layre_norm(self.dropout(att_out) + src)\n",
    "        \n",
    "        ##### out: (batch_size, seq_len, hid_dim)\n",
    "        pw_ff_out = self.positionwise_ff(pw_ff_in)\n",
    "        out = self.pw_ff_layer_norm(pw_ff_in + self.dropout(pw_ff_out))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__ (self, input_dim, n_layers, hid_dim, n_heads, pf_hid, dropout, device, max_seq_length=100):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.token_embedding = torch.nn.Embedding(input_dim, hid_dim)\n",
    "        self.position_embedding = torch.nn.Embedding(max_seq_length, hid_dim)\n",
    "        self.layers = torch.nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_hid, dropout, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        ##### src = (batch size, src len)\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        ##### pos: (batch_size, seq_len)\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        ##### src: (batch_size, seq_len, hid_dim)\n",
    "        src = self.dropout(self.token_embedding(src)*self.scale + self.position_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "src = torch.ones((5, 10), dtype=int).to(DEVICE)\n",
    "src_mask = torch.ones((5, 1, 1, 10), dtype=int).to(DEVICE)\n",
    "encoder = Encoder(15, 2, 16, 2, 64, 0.5, DEVICE, 100).to(DEVICE)\n",
    "a = encoder(src, src_mask)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6) Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_att_layer_norm = torch.nn.LayerNorm(hid_dim)\n",
    "        self.enc_att_layer_norm = torch.nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = torch.nn.LayerNorm(hid_dim)\n",
    "\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.enc_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_FF = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        ##### trg: (batch_size, trg_len, hid_dim)\n",
    "        ##### enc_src: (batch_size, src_len, hid_dim)\n",
    "        ##### src_mask: (batch_size, 1, 1, trg_len)\n",
    "        ##### trg_mask: (batch_size, 1, trg_len, trg_len)\n",
    "        ##### self_att_out: (batch_size, trg_len, hid_dim)\n",
    "        self_att_out, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        out = self.self_att_layer_norm(self.dropout(self_att_out) + trg)\n",
    "\n",
    "        enc_att_out, attention = self.enc_attention(out, enc_src, enc_src, src_mask)\n",
    "        out = self.enc_att_layer_norm(self.dropout(enc_att_out) + out)\n",
    "\n",
    "        pw_ff_out = self.positionwise_FF(out)\n",
    "        out = self.ff_layer_norm(self.dropout(pw_ff_out) + out)\n",
    "\n",
    "        return out, attention\n",
    "    \n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, n_layers, hid_dim, n_heads, pf_hid, dropout, device, max_seq_length=100):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.token_embedding = torch.nn.Embedding(output_dim, hid_dim)\n",
    "        self.position_embedding = torch.nn.Embedding(max_seq_length, hid_dim)\n",
    "        self.layers = torch.nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_hid, dropout, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out = torch.nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        ##### trg: (batch_size, trg_len)\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        ##### pos: (batch_size, seq_len)\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        ##### src: (batch_size, seq_len, hid_dim)\n",
    "        \"\"\"print(f\"pos: {pos.shape}\")\n",
    "        print(self.token_embedding(trg).shape)\n",
    "        print(self.position_embedding(pos).shape)\"\"\"\n",
    "        trg = self.dropout(self.token_embedding(trg)*self.scale + self.position_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        ##### output = (batch size, trg len, output dim)\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7) Seq2Seq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        return trg_mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #print(enc_src.shape)\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        #print(output.shape)\n",
    "\n",
    "        return output, attention\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6) Parameter initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Dim = len(src_Field.vocab)\n",
    "Output_Dim = len(trg_Field.vocab)\n",
    "Hid_Dim = 256\n",
    "Encoder_N_Layer = 3\n",
    "Decoder_N_Layer = 3\n",
    "Encoder_N_Heads = 8\n",
    "Decoder_N_Heads = 8\n",
    "Encoder_PF_Dim = 512\n",
    "Decoder_PF_Dim = 512\n",
    "Encoder_Dropout = 0.1\n",
    "Decoder_Dropout = 0.1\n",
    "Src_Pad_Idx = src_Field.vocab.stoi[src_Field.pad_token]\n",
    "Trg_Pad_Idx = trg_Field.vocab.stoi[trg_Field.pad_token]\n",
    "\n",
    "\n",
    "encoder = Encoder(input_dim=Input_Dim, n_layers=Encoder_N_Layer, hid_dim=Hid_Dim, n_heads=Encoder_N_Heads, pf_hid=Encoder_PF_Dim, dropout=Decoder_Dropout, device=DEVICE)\n",
    "decoder = Decoder(output_dim=Output_Dim, n_layers=Decoder_N_Layer, hid_dim=Hid_Dim, n_heads=Decoder_N_Heads, pf_hid=Decoder_PF_Dim, dropout=Encoder_Dropout, device=DEVICE)\n",
    "seq2seq = Seq2Seq(encoder=encoder, decoder=decoder, src_pad_idx=Src_Pad_Idx, trg_pad_idx=Trg_Pad_Idx, device=DEVICE).to(DEVICE)\n",
    "\n",
    "Learning_Rate = 0.0005\n",
    "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=Learning_Rate)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=Trg_Pad_Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "seq2seq.apply(initialize_weights);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7) Train function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimzer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        #print(src)\n",
    "        src, src_len = batch.src_field\n",
    "        trg, trg_len = batch.trg_field\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "        output = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8) Evaluation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatef(model, iterator, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, src_len = batch.src_field\n",
    "        trg, trg_len = batch.trg_field\n",
    "\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "        output = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 5.360 | Train PPL: 212.825\n",
      "\t Val. Loss: 5.228 |  Val. PPL: 186.453\n",
      "Epoch: 02\n",
      "\tTrain Loss: 4.926 | Train PPL: 137.886\n",
      "\t Val. Loss: 4.795 |  Val. PPL: 120.919\n",
      "Epoch: 03\n",
      "\tTrain Loss: 5.030 | Train PPL: 152.995\n",
      "\t Val. Loss: 5.059 |  Val. PPL: 157.397\n",
      "Epoch: 04\n",
      "\tTrain Loss: 4.967 | Train PPL: 143.526\n",
      "\t Val. Loss: 5.010 |  Val. PPL: 149.914\n",
      "Epoch: 05\n",
      "\tTrain Loss: 4.899 | Train PPL: 134.120\n",
      "\t Val. Loss: 4.941 |  Val. PPL: 139.882\n",
      "Epoch: 06\n",
      "\tTrain Loss: 4.817 | Train PPL: 123.572\n",
      "\t Val. Loss: 4.899 |  Val. PPL: 134.215\n",
      "Epoch: 07\n",
      "\tTrain Loss: 4.736 | Train PPL: 113.930\n",
      "\t Val. Loss: 4.844 |  Val. PPL: 126.942\n",
      "Epoch: 08\n",
      "\tTrain Loss: 4.671 | Train PPL: 106.833\n",
      "\t Val. Loss: 4.811 |  Val. PPL: 122.881\n",
      "Epoch: 09\n",
      "\tTrain Loss: 4.626 | Train PPL: 102.145\n",
      "\t Val. Loss: 4.780 |  Val. PPL: 119.107\n",
      "Epoch: 10\n",
      "\tTrain Loss: 4.575 | Train PPL:  97.010\n",
      "\t Val. Loss: 4.796 |  Val. PPL: 121.082\n",
      "Epoch: 11\n",
      "\tTrain Loss: 4.538 | Train PPL:  93.541\n",
      "\t Val. Loss: 4.737 |  Val. PPL: 114.113\n",
      "Epoch: 12\n",
      "\tTrain Loss: 4.482 | Train PPL:  88.368\n",
      "\t Val. Loss: 4.712 |  Val. PPL: 111.271\n",
      "Epoch: 13\n",
      "\tTrain Loss: 4.450 | Train PPL:  85.620\n",
      "\t Val. Loss: 4.816 |  Val. PPL: 123.422\n",
      "Epoch: 14\n",
      "\tTrain Loss: 4.351 | Train PPL:  77.523\n",
      "\t Val. Loss: 4.476 |  Val. PPL:  87.841\n",
      "Epoch: 15\n",
      "\tTrain Loss: 4.039 | Train PPL:  56.744\n",
      "\t Val. Loss: 4.342 |  Val. PPL:  76.860\n",
      "Epoch: 16\n",
      "\tTrain Loss: 3.983 | Train PPL:  53.685\n",
      "\t Val. Loss: 4.353 |  Val. PPL:  77.731\n",
      "Epoch: 17\n",
      "\tTrain Loss: 3.929 | Train PPL:  50.848\n",
      "\t Val. Loss: 4.311 |  Val. PPL:  74.508\n",
      "Epoch: 18\n",
      "\tTrain Loss: 3.889 | Train PPL:  48.841\n",
      "\t Val. Loss: 4.276 |  Val. PPL:  71.935\n",
      "Epoch: 19\n",
      "\tTrain Loss: 3.843 | Train PPL:  46.665\n",
      "\t Val. Loss: 4.257 |  Val. PPL:  70.580\n",
      "Epoch: 20\n",
      "\tTrain Loss: 3.795 | Train PPL:  44.471\n",
      "\t Val. Loss: 4.217 |  Val. PPL:  67.852\n",
      "Epoch: 21\n",
      "\tTrain Loss: 3.743 | Train PPL:  42.235\n",
      "\t Val. Loss: 4.224 |  Val. PPL:  68.324\n",
      "Epoch: 22\n",
      "\tTrain Loss: 3.732 | Train PPL:  41.759\n",
      "\t Val. Loss: 4.200 |  Val. PPL:  66.669\n",
      "Epoch: 23\n",
      "\tTrain Loss: 3.698 | Train PPL:  40.382\n",
      "\t Val. Loss: 4.184 |  Val. PPL:  65.627\n",
      "Epoch: 24\n",
      "\tTrain Loss: 3.666 | Train PPL:  39.111\n",
      "\t Val. Loss: 4.224 |  Val. PPL:  68.302\n",
      "Epoch: 25\n",
      "\tTrain Loss: 3.650 | Train PPL:  38.457\n",
      "\t Val. Loss: 4.182 |  Val. PPL:  65.519\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 25\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):    \n",
    "    train_loss = train(seq2seq, train_Iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluatef(seq2seq, test_Iterator, criterion)\n",
    "            \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(seq2seq.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **9) Computing Metrics (Bleu Score, Rouge, WER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [03:07<00:00,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average Testing Bleu Score is: 0.0015744735271415128\n",
      "The Average Testing Rouge-1 Score is: 0.25114728547952536\n",
      "The Average Testing Rouge-2 Score is: 0.028272034809479856\n",
      "The Average Testing Rouge-L Score is: 0.22628870846405386\n",
      "The Average Testing Rouge-Lsum Score is: 0.22628870846405386\n",
      "Average Word Error Rate: 0.7800511333513602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to translate a sentence using argmax\n",
    "def translate_sentence(model, sentence, trg_sent, src_field, tgt_field, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    tokens = src_field.tokenize(sentence)\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    tokens = tgt_field.tokenize(trg_sent)\n",
    "    tokens = [tgt_field.init_token] + tokens + [tgt_field.eos_token]\n",
    "    src_indexes = [tgt_field.vocab.stoi[token] for token in tokens]\n",
    "    trg_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    trg_mask = model.make_trg_mask(trg_tensor)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"for i in range(max_len):\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _ = model.decoder(trg_tensor, encoder_outputs, trg_mask, src_mask)\n",
    "\n",
    "\n",
    "    output = output.squeeze()\n",
    "\n",
    "    pred_token = output.argmax(1)\n",
    "\n",
    "\n",
    "    \"\"\"if pred_token == tgt_field.vocab.stoi[tgt_field.eos_token]:\n",
    "        break\"\"\"\n",
    "    \n",
    "    trg_indexes = list(np.array(pred_token.cpu()))\n",
    "\n",
    "    trg_tokens = [tgt_field.vocab.itos[i] for i in trg_indexes[:-1]]\n",
    "    return trg_tokens[1:-1], trg_indexes[1:-1]\n",
    "\n",
    "# Translate each test data point using argmax and print the English sentence, the ground truth Dutch translation, and the model's generated Dutch translation\n",
    "predicted_output = []\n",
    "ground_truth_output = []\n",
    "bleu = load_metric(\"bleu\") \n",
    "rouge = evaluate.load('rouge')\n",
    "Bleu_total = 0\n",
    "rouge1_total = 0\n",
    "rouge2_total = 0\n",
    "rougeL_total = 0\n",
    "rougeLsum_total = 0\n",
    "WER_total = 0\n",
    "\n",
    "n = 0\n",
    "for batch in tqdm(valid_Iterator):\n",
    "    src = batch.src_field[0]\n",
    "    trg = batch.trg_field[0]\n",
    "\n",
    "\n",
    "\n",
    "    for idx in range(src.shape[0]):\n",
    "\n",
    "        src_sent = ' '.join([src_Field.vocab.itos[i] for i in src[idx, 1:src.shape[1]-1]])\n",
    "        trg_sent = ' '.join([trg_Field.vocab.itos[i] for i in trg[idx, 1:trg.shape[1]-1]])\n",
    "        trg_sent = trg_sent.split(\" \")\n",
    "        final_trg = [] \n",
    "        for w in trg_sent:\n",
    "            if w != '<pad>' and w != '<eos>':\n",
    "                final_trg.append(w)\n",
    "\n",
    "        \n",
    "        \n",
    "        translated_sent, _ = translate_sentence(seq2seq, src_sent, ' '.join(final_trg), src_Field, trg_Field, DEVICE)\n",
    "\n",
    "        final_trg = [] \n",
    "        for w in trg_sent:\n",
    "            if w != '<pad>' and w != '<eos>':\n",
    "                final_trg.append(w)\n",
    "        final_trg = \" \".join(w for w in final_trg)\n",
    "        \n",
    "        \"\"\"print(f\"English: {src_sent}\")\n",
    "        print(f\"Ground Truth Dutch: {final_trg}\")\n",
    "        print(f\"Generated Dutch: {' '.join(translated_sent)}\")\"\"\"\n",
    "        predicted_output = translated_sent\n",
    "        ground_truth_output = final_trg.split()\n",
    "\n",
    "        ## Computing the Rouge\n",
    "        results = rouge.compute(predictions=[' '.join(predicted_output)], references=[final_trg])\n",
    "        rouge1_total += results['rouge1']\n",
    "        rouge2_total += results['rouge2']\n",
    "        rougeL_total += results['rougeL']\n",
    "        rougeLsum_total += results['rougeLsum']\n",
    "\n",
    "        ## Computing the Word Error Rate\n",
    "        WER_total += wer(final_trg, ' '.join(predicted_output))\n",
    "\n",
    "        ## Computing the BLEU score\n",
    "        predicted_output = [predicted_output]\n",
    "        ground_truth_output = [[ground_truth_output]]\n",
    "\n",
    "        Bleu_total += bleu.compute(predictions=predicted_output, references=ground_truth_output)['bleu']\n",
    "        n += 1\n",
    "\n",
    "\n",
    "\n",
    "print(f\"The Average Testing Bleu Score is: {Bleu_total / n}\")\n",
    "print(f\"The Average Testing Rouge-1 Score is: {rouge1_total / n}\")\n",
    "print(f\"The Average Testing Rouge-2 Score is: {rouge2_total / n}\")\n",
    "print(f\"The Average Testing Rouge-L Score is: {rougeL_total / n}\")\n",
    "print(f\"The Average Testing Rouge-Lsum Score is: {rougeLsum_total / n}\")\n",
    "print(f\"Average Word Error Rate: {WER_total / n}\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pyth310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
